
@article{krompas_exploiting_2015,
	title = {Exploiting {Latent} {Embeddings} of {Nominal} {Clinical} {Data} for {Predicting} {Hospital} {Readmission}},
	volume = {29},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/10.1007/s13218-014-0344-x},
	doi = {10.1007/s13218-014-0344-x},
	language = {en},
	number = {2},
	urldate = {2023-12-06},
	journal = {KI - Künstliche Intelligenz},
	author = {Krompaß, Denis and Esteban, Cristóbal and Tresp, Volker and Sedlmayr, Martin and Ganslandt, Thomas},
	month = jun,
	year = {2015},
	pages = {153--159},
}

@article{glance_tmpmicd9_2009,
	title = {{TMPM}–{ICD9}: {A} {Trauma} {Mortality} {Prediction} {Model} {Based} on {ICD}-9-{CM} {Codes}},
	volume = {249},
	issn = {0003-4932},
	shorttitle = {{TMPM}–{ICD9}},
	url = {https://journals.lww.com/00000658-200906000-00025},
	doi = {10.1097/SLA.0b013e3181a38f28},
	language = {en},
	number = {6},
	urldate = {2023-12-06},
	journal = {Annals of Surgery},
	author = {Glance, Laurent G. and Osler, Turner M. and Mukamel, Dana B. and Meredith, Wayne and Wagner, Jacob and Dick, Andrew W.},
	month = jun,
	year = {2009},
	pages = {1032--1039},
}

@inproceedings{assaf_30-day_2020,
	address = {Tashkent, Uzbekistan},
	title = {30-day {Hospital} {Readmission} {Prediction} using {MIMIC} {Data}},
	isbn = {978-1-72817-386-3},
	url = {https://ieeexplore.ieee.org/document/9368625/},
	doi = {10.1109/AICT50176.2020.9368625},
	urldate = {2023-12-06},
	booktitle = {2020 {IEEE} 14th {International} {Conference} on {Application} of {Information} and {Communication} {Technologies} ({AICT})},
	publisher = {IEEE},
	author = {Assaf, Rasha and Jayousi, Rashid},
	month = oct,
	year = {2020},
	pages = {1--6},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
	doi = {10.1037/h0042519},
	language = {en},
	number = {6},
	urldate = {2023-12-06},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {386--408},
}

@misc{murphy_janossy_2019,
	title = {Janossy {Pooling}: {Learning} {Deep} {Permutation}-{Invariant} {Functions} for {Variable}-{Size} {Inputs}},
	shorttitle = {Janossy {Pooling}},
	url = {http://arxiv.org/abs/1811.01900},
	abstract = {We consider a simple and overarching representation for permutation-invariant functions of sequences (or multiset functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with \$k\$-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Murphy, Ryan L. and Srinivasan, Balasubramaniam and Rao, Vinayak and Ribeiro, Bruno},
	month = feb,
	year = {2019},
	note = {arXiv:1811.01900 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: This version clarifies and adds detail to some of the arguments},
}

@misc{yuan_code_2022,
	title = {Code {Synonyms} {Do} {Matter}: {Multiple} {Synonyms} {Matching} {Network} for {Automatic} {ICD} {Coding}},
	shorttitle = {Code {Synonyms} {Do} {Matter}},
	url = {http://arxiv.org/abs/2203.01515},
	abstract = {Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs). Existing methods usually apply label attention with code representations to match related text snippets. Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Yuan, Zheng and Tan, Chuanqi and Huang, Songfang},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01515 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ACL 2022 Main Conference, Short Paper},
}

@article{turner_inhibition_1975,
	title = {Inhibition of aldehyde reductase by acidic metabolites of the biogenic amines},
	volume = {24},
	issn = {0006-2952},
	doi = {10.1016/0006-2952(75)90016-7},
	language = {eng},
	number = {18},
	journal = {Biochemical Pharmacology},
	author = {Turner, A. J. and Hick, P. E.},
	month = sep,
	year = {1975},
	pmid = {16},
	keywords = {Aldehyde Oxidoreductases, Animals, Biogenic Amines, Brain, Homovanillic Acid, In Vitro Techniques, Kinetics, NADP, Pyrimidines, Sheep},
	pages = {1731--1733},
}

@article{hancock_evaluating_2023,
	title = {Evaluating classifier performance with highly imbalanced {Big} {Data}},
	volume = {10},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00724-5},
	doi = {10.1186/s40537-023-00724-5},
	abstract = {Abstract
            Using the wrong metrics to gauge classification of highly imbalanced Big Data may hide important information in experimental results. However, we find that analysis of metrics for performance evaluation and what they can hide or reveal is rarely covered in related works. Therefore, we address that gap by analyzing multiple popular performance metrics on three Big Data classification tasks. To the best of our knowledge, we are the first to utilize three new Medicare insurance claims datasets which became publicly available in 2021. These datasets are all highly imbalanced. Furthermore, the datasets are comprised of completely different data. We evaluate the performance of five ensemble learners in the Machine Learning task of Medicare fraud detection. Random Undersampling (RUS) is applied to induce five class ratios. The classifiers are evaluated with both the Area Under the Receiver Operating Characteristic Curve (AUC), and Area Under the Precision Recall Curve (AUPRC) metrics. We show that AUPRC provides a better insight into classification performance. Our findings reveal that the AUC metric hides the performance impact of RUS. However, classification results in terms of AUPRC show RUS has a detrimental effect. We show that, for highly imbalanced Big Data, the AUC metric fails to capture information about precision scores and false positive counts that the AUPRC metric reveals. Our contribution is to show AUPRC is a more effective metric for evaluating the performance of classifiers when working with highly imbalanced Big Data.},
	language = {en},
	number = {1},
	urldate = {2023-12-06},
	journal = {Journal of Big Data},
	author = {Hancock, John T. and Khoshgoftaar, Taghi M. and Johnson, Justin M.},
	month = apr,
	year = {2023},
	pages = {42},
}

@article{dai_analysis_2020,
	title = {Analysis of adult disease characteristics and mortality on {MIMIC}-{III}},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0232176},
	doi = {10.1371/journal.pone.0232176},
	language = {en},
	number = {4},
	urldate = {2023-12-06},
	journal = {PLOS ONE},
	author = {Dai, Zheng and Liu, Siru and Wu, Jinfa and Li, Mengdie and Liu, Jialin and Li, Ke},
	editor = {Beiki, Omid},
	month = apr,
	year = {2020},
	pages = {e0232176},
}

@article{scherpf_predicting_2019,
	title = {Predicting sepsis with a recurrent neural network using the {MIMIC} {III} database},
	volume = {113},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482519302720},
	doi = {10.1016/j.compbiomed.2019.103395},
	language = {en},
	urldate = {2023-12-06},
	journal = {Computers in Biology and Medicine},
	author = {Scherpf, Matthieu and Gräßer, Felix and Malberg, Hagen and Zaunseder, Sebastian},
	month = oct,
	year = {2019},
	pages = {103395},
}

@article{cowper-smith_communication_1979,
	title = {Communication at {Naidex} '79},
	volume = {75},
	issn = {0954-7762},
	language = {eng},
	number = {49},
	journal = {Nursing Times},
	author = {Cowper-Smith, F.},
	month = dec,
	year = {1979},
	pmid = {160035},
	keywords = {Communication Aids for Disabled, England, Exhibitions as Topic, Humans, Self-Help Devices},
	pages = {2100--2102},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
}

@misc{zaheer_deep_2018,
	title = {Deep {Sets}},
	url = {http://arxiv.org/abs/1703.06114},
	abstract = {We study the problem of designing models for machine learning tasks defined on {\textbackslash}emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics {\textbackslash}cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams {\textbackslash}cite\{Jung15Exploration\}, to cosmology {\textbackslash}cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
	month = apr,
	year = {2018},
	note = {arXiv:1703.06114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2017},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv:1310.4546 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_national_nodate,
	title = {National {Trends} in {Hospital} and {Physician} {Adoption} of {Electronic} {Health} {Records} {\textbar} {HealthIT}.gov},
	url = {https://www.healthit.gov/data/quickstats/national-trends-hospital-and-physician-adoption-electronic-health-records},
	urldate = {2023-12-06},
}

@misc{noauthor_improved_nodate,
	title = {Improved {Diagnostics} \& {Patient} {Outcomes} {\textbar} {HealthIT}.gov},
	url = {https://www.healthit.gov/topic/health-it-and-health-information-exchange-basics/improved-diagnostics-patient-outcomes},
	urldate = {2023-12-06},
}

@misc{noauthor_importance_nodate,
	title = {Importance of {ICD}},
	url = {https://www.who.int/standards/classifications/frequently-asked-questions/importance-of-icd},
	language = {en},
	urldate = {2023-12-06},
}

@misc{noauthor_icd_2023,
	title = {{ICD} - {ICD}-9 - {International} {Classification} of {Diseases}, {Ninth} {Revision}},
	url = {https://www.cdc.gov/nchs/icd/icd9.htm},
	language = {en-us},
	urldate = {2023-12-06},
	month = jun,
	year = {2023},
}

@misc{noauthor_what_nodate,
	title = {What is {ICD}-9 {Coding}?},
	url = {https://www.the-rheumatologist.org/article/what-is-icd-9-coding/},
	abstract = {The International Classification of Diseases Clinical Modification, 9th Revision (ICD-9 CM) is a list of codes intended for the classification of diseases and a wide variety of signs, symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or disease. The numerical format of the diagnosis codes usually ranges from three to five digits that are assigned to a unique category.},
	language = {en-US},
	urldate = {2023-12-06},
	journal = {The Rheumatologist},
}

@incollection{ehrenstein_obtaining_2019,
	title = {Obtaining {Data} {From} {Electronic} {Health} {Records}},
	url = {https://www.ncbi.nlm.nih.gov/books/NBK551878/},
	abstract = {There is growing interest in using data captured in electronic health records (EHRs) for patient registries. Both EHRs and patient registries capture and use patient-level clinical information, but conceptually, they are designed for different purposes. A patient registry is defined as “an organized system that uses observational study methods to collect uniform data (clinical and other) to evaluate specified outcomes for a population defined by a particular disease, condition, or exposure and that serves one or more predetermined scientific, clinical, or policy purposes.”1},
	language = {en},
	urldate = {2023-12-06},
	booktitle = {Tools and {Technologies} for {Registry} {Interoperability}, {Registries} for {Evaluating} {Patient} {Outcomes}: {A} {User}’s {Guide}, 3rd {Edition}, {Addendum} 2 [{Internet}]},
	publisher = {Agency for Healthcare Research and Quality (US)},
	author = {Ehrenstein, Vera and Kharrazi, Hadi and Lehmann, Harold and Taylor, Casey Overby},
	month = oct,
	year = {2019},
}

@article{ayala_solares_deep_2020,
	title = {Deep learning for electronic health records: {A} comparative review of multiple deep neural architectures},
	volume = {101},
	issn = {1532-0464},
	shorttitle = {Deep learning for electronic health records},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046419302564},
	doi = {10.1016/j.jbi.2019.103337},
	abstract = {Despite the recent developments in deep learning models, their applications in clinical decision-support systems have been very limited. Recent digitalisation of health records, however, has provided a great platform for the assessment of the usability of such techniques in healthcare. As a result, the field is starting to see a growing number of research papers that employ deep learning on electronic health records (EHR) for personalised prediction of risks and health trajectories. While this can be a promising trend, vast paper-to-paper variability (from data sources and models they use to the clinical questions they attempt to answer) have hampered the field’s ability to simply compare and contrast such models for a given application of interest. Thus, in this paper, we aim to provide a comparative review of the key deep learning architectures that have been applied to EHR data. Furthermore, we also aim to: (1) introduce and use one of the world’s largest and most complex linked primary care EHR datasets (i.e., Clinical Practice Research Datalink, or CPRD) as a new asset for training such data-hungry models; (2) provide a guideline for working with EHR data for deep learning; (3) share some of the best practices for assessing the “goodness” of deep-learning models in clinical risk prediction; (4) and propose future research ideas for making deep learning models more suitable for the EHR data. Our results highlight the difficulties of working with highly imbalanced datasets, and show that sequential deep learning architectures such as RNN may be more suitable to deal with the temporal nature of EHR.},
	urldate = {2023-12-06},
	journal = {Journal of Biomedical Informatics},
	author = {Ayala Solares, Jose Roberto and Diletta Raimondi, Francesca Elisa and Zhu, Yajie and Rahimian, Fatemeh and Canoy, Dexter and Tran, Jenny and Pinho Gomes, Ana Catarina and Payberah, Amir H. and Zottoli, Mariagrazia and Nazarzadeh, Milad and Conrad, Nathalie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
	month = jan,
	year = {2020},
	keywords = {CPRD, Deep learning, Electronic health records, Neural networks, Representation learning},
	pages = {103337},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {Dropout},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	month = jan,
	year = {2014},
	keywords = {deep learning, model combination, neural networks, regularization},
	pages = {1929--1958},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {http://arxiv.org/abs/1106.1813},
	doi = {10.1613/jair.953},
	abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
	urldate = {2023-12-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	note = {arXiv:1106.1813 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {321--357},
}
