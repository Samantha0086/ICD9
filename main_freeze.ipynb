{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "from model_freeze import Word2Vec_neg_sampling\n",
    "from utils_modified import count_parameters\n",
    "from datasets import word2vec_dataset\n",
    "\n",
    "from helper import evaluate,data_loader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = [\"MORTALITY_30_DAY\", \"MORTALITY_1_YEAR\", \"READMISSION_30_DAY\", \"READMISSION_1_YEAR\"][3]\n",
    "predictor =[\"PROCEDURE_ICD\", \"DIAGNOSIS_ICD\", \"PROCEDURE_AND_DIAGNOSIS_ICD\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"admit_modified.csv\")[[outcome,'LOS', 'AGE', 'GENDER_M', \"ETHNICITY_Asian\", \n",
    "     \"ETHNICITY_Black\", \"ETHNICITY_Hispanic\", \"ETHNICITY_Native_Hawaiian\", \"ETHNICITY_Other\", \n",
    "     \"ETHNICITY_White\", predictor]]\n",
    "X=X.dropna()\n",
    "X = X.reset_index().drop(columns = [\"index\"])\n",
    "y = X[outcome].values\n",
    "X = X.drop(columns = outcome)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/wprlk9ys2c357_3d270bzhzw0000gn/T/ipykernel_24474/878727298.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[predictor][i] = X[predictor][i].replace(\"'\", \"\")[1:-1].split(\", \")\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)): \n",
    "    X[predictor][i] = X[predictor][i].replace(\"'\", \"\")[1:-1].split(\", \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 1, stratify = y) #stratify\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\n",
    "train_index = X_train.index\n",
    "test_index = X_test.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-Gram\n",
    "\n",
    "NEGATIVE_SAMPLES = 20\n",
    "LR                    = 0.001\n",
    "\n",
    "\n",
    "BATCH_SIZE            = 256\n",
    "batch_size = 2**5\n",
    "\n",
    "NUM_EPOCHS            = 30 #int(1e+3)  \n",
    "\n",
    "weight_cnn = 0.8\n",
    "EMBEDDING_DIM = 200\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# add other variables with ICD code together\n",
    "class_weights = torch.tensor(compute_class_weight( class_weight =\"balanced\", classes =  np.unique(y_train),y =  y_train ), dtype = torch.float)\n",
    "criterion_cnn = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_word_freqs\n",
      "gather word freqs takes 0.5391 seconds\n",
      "gather training data\n",
      "gather training data takes 2.2329 seconds\n",
      "encode beginning\n",
      "encode takes 0.2538 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('dataset.pkl', 'wb') as outp:\\n    \\n    pickle.dump(dataset, outp, pickle.HIGHEST_PROTOCOL)\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = word2vec_dataset(predictor, X, train_index, test_index)\n",
    "\n",
    "'''\n",
    "with open('dataset.pkl', 'wb') as outp:\n",
    "    \n",
    "    pickle.dump(dataset, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.drop(columns = predictor).dropna()\n",
    "X_test = X_test.drop(columns = predictor).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset):  35898\n",
      "len(train_loader_sg):  278202\n",
      "len(train_dataloader):  110\n",
      "len(vocab):  7657 \n",
      "\n",
      "\n",
      "We have 3.12012 Million trainable parameters here in the word2vec\n"
     ]
    }
   ],
   "source": [
    "# takes time\n",
    "vocab = dataset.vocab\n",
    "\n",
    "word_to_ix = dataset.word_to_ix\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_ix.keys())\n",
    "embedding_dict = word_to_ix\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader =  data_loader( dataset.code_same_len[train_index],  np.array(X_train), dataset.code_same_len[test_index], np.array(X_test), y_train, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_loader_sg = torch.utils.data.DataLoader(dataset.training_data, batch_size = batch_size, shuffle = not True)\n",
    "test_loader_sg = torch.utils.data.DataLoader(dataset.testing_data, batch_size = batch_size, shuffle = not True)\n",
    "\n",
    "print('len(dataset): ', len(dataset))\n",
    "print('len(train_loader_sg): ', len(train_loader_sg))\n",
    "print('len(train_dataloader): ', len(train_dataloader))\n",
    "print('len(vocab): ', len(vocab), '\\n')\n",
    "\n",
    "\n",
    "# make noise distribution to sample negative examples from\n",
    "word_freqs = np.array(list(vocab))\n",
    "unigram_dist = word_freqs/sum(word_freqs)\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "word2vec = Word2Vec_neg_sampling(EMBEDDING_DIM, len(vocab), DEVICE, noise_dist, NEGATIVE_SAMPLES).to(DEVICE)\n",
    "print('\\nWe have {} Million trainable parameters here in the word2vec'.format(count_parameters(word2vec)))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(word2vec.parameters(), lr = LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EPOCH 1/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 8.54511038606817 \n",
      "\n",
      "average train accuracy 67.65151515151516 \n",
      "\n",
      "train auroc 68.72041833335021 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.792973656808176 \n",
      "\n",
      "val accuracy 79.73767201834863 \n",
      "\n",
      "val auroc 73.56511062140169 \n",
      "\n",
      "val auprc 36.87181334841332 \n",
      "\n",
      "\n",
      "===== EPOCH 2/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 8.253389687971636 \n",
      "\n",
      "average train accuracy 70.32907196969697 \n",
      "\n",
      "train auroc 71.27443575280066 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.629905008500621 \n",
      "\n",
      "val accuracy 69.58512133767387 \n",
      "\n",
      "val auroc 74.63472438628732 \n",
      "\n",
      "val auprc 37.49057553314195 \n",
      "\n",
      "\n",
      "===== EPOCH 3/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 7.910826284235174 \n",
      "\n",
      "average train accuracy 71.95785984848484 \n",
      "\n",
      "train auroc 73.72796619251308 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.521786782049364 \n",
      "\n",
      "val accuracy 67.93823061556674 \n",
      "\n",
      "val auroc 74.52997474030781 \n",
      "\n",
      "val auprc 36.80939796166609 \n",
      "\n",
      "\n",
      "===== EPOCH 4/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 7.631595407832752 \n",
      "\n",
      "average train accuracy 73.7606534090909 \n",
      "\n",
      "train auroc 75.09920865441185 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.319971438377134 \n",
      "\n",
      "val accuracy 66.41734795797574 \n",
      "\n",
      "val auroc 74.55177026134048 \n",
      "\n",
      "val auprc 36.76083578205071 \n",
      "\n",
      "\n",
      "===== EPOCH 5/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 7.234940997037021 \n",
      "\n",
      "average train accuracy 75.09232954545455 \n",
      "\n",
      "train auroc 75.96650819300297 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.05331750069895 \n",
      "\n",
      "val accuracy 64.03244303048238 \n",
      "\n",
      "val auroc 74.30628407790275 \n",
      "\n",
      "val auprc 36.86277761795409 \n",
      "\n",
      "\n",
      "===== EPOCH 6/30 =====\n",
      "\n",
      "TRAINING...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('\\n===== EPOCH {}/{} ====='.format(epoch + 1, NUM_EPOCHS))    \n",
    "    print('\\nTRAINING...')\n",
    "\n",
    "    train_accuracy = [ ]\n",
    "    train_auroc = []\n",
    "    \n",
    "    train_loss = []\n",
    "    word2vec.train()\n",
    "    \n",
    "    for item1, item2 in zip(train_loader_sg, train_dataloader): \n",
    "        x_batch = item1[:,0]\n",
    "        y_batch = item1[:,1]\n",
    "        \n",
    "        # X is the input ids and X1 is other features\n",
    "        X, X1, y = item2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        loss_word2vec, logits = word2vec(x_batch, y_batch, X, X1)\n",
    "\n",
    "        \n",
    "        loss_cnn = criterion_cnn(logits,y)\n",
    "        loss =( 1-weight_cnn)*loss_word2vec + weight_cnn* loss_cnn  # * weight\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()    \n",
    "        \n",
    "                \n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        proba =logits[:,1].detach().numpy()\n",
    "\n",
    "\n",
    "        accuracy = (preds == y).cpu().numpy().mean() * 100\n",
    "        \n",
    "        \n",
    "        auroc =  roc_auc_score(y, proba)\n",
    "        \n",
    "        train_accuracy.append(accuracy)\n",
    "        train_auroc.append(auroc)\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"average train loss\", np.mean(train_loss), '\\n')\n",
    "    print(\"average train accuracy\", np.mean(train_accuracy), '\\n')\n",
    "    print(\"train auroc\", np.mean(train_auroc)*100, '\\n')\n",
    "    \n",
    "   \n",
    "    print(\"VALIDATION... \\n\")\n",
    "    val_loss, val_accuracy, val_auroc, val_auprc = evaluate(word2vec, val_dataloader, test_loader_sg, word2vec, criterion_cnn, weight_cnn)\n",
    "    print(\"val loss\", val_loss, '\\n')\n",
    "    print(\"val accuracy\", val_accuracy, '\\n')\n",
    "    print(\"val auroc\", np.mean(val_auroc)*100, '\\n')\n",
    "    print(\"val auprc\", np.mean(val_auprc)*100, '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
