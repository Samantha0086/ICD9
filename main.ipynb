{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "from model import Word2Vec_neg_sampling\n",
    "from utils_modified import count_parameters\n",
    "from datasets import word2vec_dataset\n",
    "\n",
    "from helper import evaluate,data_loader\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = [\"MORTALITY_30_DAY\", \"MORTALITY_1_YEAR\", \"READMISSION_30_DAY\", \"READMISSION_1_YEAR\"][3]\n",
    "predictor =[\"PROCEDURE_ICD\", \"DIAGNOSIS_ICD\", \"PROCEDURE_AND_DIAGNOSIS_ICD\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"admit_modified.csv\")[[outcome,'LOS', 'AGE', 'GENDER_M', \"ETHNICITY_Asian\", \n",
    "     \"ETHNICITY_Black\", \"ETHNICITY_Hispanic\", \"ETHNICITY_Native_Hawaiian\", \"ETHNICITY_Other\", \n",
    "     \"ETHNICITY_White\", predictor]]\n",
    "X=X.dropna()\n",
    "X = X.reset_index().drop(columns = [\"index\"])\n",
    "y = X[outcome].values\n",
    "X = X.drop(columns = outcome)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4134129/878727298.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[predictor][i] = X[predictor][i].replace(\"'\", \"\")[1:-1].split(\", \")\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)): \n",
    "    X[predictor][i] = X[predictor][i].replace(\"'\", \"\")[1:-1].split(\", \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 1, stratify = y) #stratify\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\n",
    "train_index = X_train.index\n",
    "test_index = X_test.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-Gram\n",
    "\n",
    "NEGATIVE_SAMPLES = 20\n",
    "LR                    = 0.001\n",
    "\n",
    "\n",
    "BATCH_SIZE            = 256\n",
    "batch_size = 2**5\n",
    "\n",
    "NUM_EPOCHS            = 30 #int(1e+3)  \n",
    "\n",
    "weight_cnn = 0.8\n",
    "EMBEDDING_DIM = 200\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# add other variables with ICD code together\n",
    "class_weights = torch.tensor(compute_class_weight( class_weight =\"balanced\", classes =  np.unique(y_train),y =  y_train ), dtype = torch.float)\n",
    "criterion_cnn = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_word_freqs\n",
      "gather word freqs takes 1.1895 seconds\n",
      "gather training data\n",
      "max code count is  71\n",
      "gather training data takes 1.6905 seconds\n",
      "encode beginning\n",
      "encode takes 0.2172 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('dataset.pkl', 'wb') as outp:\\n    \\n    pickle.dump(dataset, outp, pickle.HIGHEST_PROTOCOL)\\n\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = word2vec_dataset(predictor, X, train_index, test_index)\n",
    "\n",
    "'''\n",
    "with open('dataset.pkl', 'wb') as outp:\n",
    "    \n",
    "    pickle.dump(dataset, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.drop(columns = predictor).dropna()\n",
    "X_test = X_test.drop(columns = predictor).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset):  35898\n",
      "len(train_loader_sg):  278202\n",
      "len(train_dataloader):  110\n",
      "len(vocab):  7657 \n",
      "\n",
      "\n",
      "We have 4.85912 Million trainable parameters here in the word2vec\n"
     ]
    }
   ],
   "source": [
    "# takes time\n",
    "vocab = dataset.vocab\n",
    "\n",
    "word_to_ix = dataset.word_to_ix\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_ix.keys())\n",
    "embedding_dict = word_to_ix\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader =  data_loader( dataset.code_same_len[train_index],  np.array(X_train), dataset.code_same_len[test_index], np.array(X_test), y_train, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_loader_sg = torch.utils.data.DataLoader(dataset.training_data, batch_size = batch_size, shuffle = not True)\n",
    "test_loader_sg = torch.utils.data.DataLoader(dataset.testing_data, batch_size = batch_size, shuffle = not True)\n",
    "\n",
    "print('len(dataset): ', len(dataset))\n",
    "print('len(train_loader_sg): ', len(train_loader_sg))\n",
    "print('len(train_dataloader): ', len(train_dataloader))\n",
    "print('len(vocab): ', len(vocab), '\\n')\n",
    "\n",
    "\n",
    "# make noise distribution to sample negative examples from\n",
    "word_freqs = np.array(list(vocab))\n",
    "unigram_dist = word_freqs/sum(word_freqs)\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "word2vec = Word2Vec_neg_sampling(EMBEDDING_DIM, len(vocab), DEVICE, noise_dist, NEGATIVE_SAMPLES).to(DEVICE)\n",
    "print('\\nWe have {} Million trainable parameters here in the word2vec'.format(count_parameters(word2vec)))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(word2vec.parameters(), lr = LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EPOCH 1/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 8.972086867419156 \n",
      "\n",
      "average train accuracy 57.3046875 \n",
      "\n",
      "train auroc 66.44140412072757 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.757800994380828 \n",
      "\n",
      "val accuracy 64.99623131843741 \n",
      "\n",
      "val auroc 73.76559690484532 \n",
      "\n",
      "\n",
      "===== EPOCH 2/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 8.536856157129462 \n",
      "\n",
      "average train accuracy 68.69910037878788 \n",
      "\n",
      "train auroc 76.86581803851617 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.662410413065265 \n",
      "\n",
      "val accuracy 72.5896622521456 \n",
      "\n",
      "val auroc 75.19715706451692 \n",
      "\n",
      "\n",
      "===== EPOCH 3/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 8.213747111233799 \n",
      "\n",
      "average train accuracy 72.85984848484848 \n",
      "\n",
      "train auroc 81.9460283584647 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.54444734511837 \n",
      "\n",
      "val accuracy 70.09354653743712 \n",
      "\n",
      "val auroc 75.40056539352801 \n",
      "\n",
      "\n",
      "===== EPOCH 4/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 7.759079998189753 \n",
      "\n",
      "average train accuracy 76.69507575757576 \n",
      "\n",
      "train auroc 85.9300717145436 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.33253611287763 \n",
      "\n",
      "val accuracy 75.33663805859722 \n",
      "\n",
      "val auroc 75.04947793720645 \n",
      "\n",
      "\n",
      "===== EPOCH 5/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 7.3652023055336695 \n",
      "\n",
      "average train accuracy 80.05089962121212 \n",
      "\n",
      "train auroc 89.16679791597763 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.08796553457937 \n",
      "\n",
      "val accuracy 73.07831921426457 \n",
      "\n",
      "val auroc 74.61039411734698 \n",
      "\n",
      "\n",
      "===== EPOCH 6/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 6.906492202932184 \n",
      "\n",
      "average train accuracy 83.18655303030303 \n",
      "\n",
      "train auroc 91.88290745720103 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 8.022104740142822 \n",
      "\n",
      "val accuracy 77.72755437999409 \n",
      "\n",
      "val auroc 73.70304476169865 \n",
      "\n",
      "\n",
      "===== EPOCH 7/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 6.513756249167702 \n",
      "\n",
      "average train accuracy 85.71969696969697 \n",
      "\n",
      "train auroc 93.88776466712878 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.94081780218309 \n",
      "\n",
      "val accuracy 70.82219684078129 \n",
      "\n",
      "val auroc 73.3172900788824 \n",
      "\n",
      "\n",
      "===== EPOCH 8/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 6.158583827452226 \n",
      "\n",
      "average train accuracy 88.50023674242424 \n",
      "\n",
      "train auroc 95.97825866636207 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.8573773907076925 \n",
      "\n",
      "val accuracy 80.07847181118674 \n",
      "\n",
      "val auroc 71.80087230982909 \n",
      "\n",
      "\n",
      "===== EPOCH 9/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 5.6835852688009085 \n",
      "\n",
      "average train accuracy 90.5658143939394 \n",
      "\n",
      "train auroc 97.01213947249042 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.686307968631867 \n",
      "\n",
      "val accuracy 77.5385422832199 \n",
      "\n",
      "val auroc 71.05573518371571 \n",
      "\n",
      "\n",
      "===== EPOCH 10/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 5.247036144950173 \n",
      "\n",
      "average train accuracy 91.58380681818181 \n",
      "\n",
      "train auroc 97.57031796488114 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.656944290284188 \n",
      "\n",
      "val accuracy 81.09913713376739 \n",
      "\n",
      "val auroc 70.71385296172788 \n",
      "\n",
      "\n",
      "===== EPOCH 11/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 4.80015246217901 \n",
      "\n",
      "average train accuracy 93.21614583333333 \n",
      "\n",
      "train auroc 98.27808880816484 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.318628480357509 \n",
      "\n",
      "val accuracy 78.36799903817698 \n",
      "\n",
      "val auroc 69.97254518471513 \n",
      "\n",
      "\n",
      "===== EPOCH 12/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 4.482143837755377 \n",
      "\n",
      "average train accuracy 94.30279356060605 \n",
      "\n",
      "train auroc 98.73601379561613 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.233056652930475 \n",
      "\n",
      "val accuracy 77.80315921870377 \n",
      "\n",
      "val auroc 70.1041929719093 \n",
      "\n",
      "\n",
      "===== EPOCH 13/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 4.008913690393621 \n",
      "\n",
      "average train accuracy 94.58570075757576 \n",
      "\n",
      "train auroc 98.92471785895714 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.324480456690634 \n",
      "\n",
      "val accuracy 79.092834418467 \n",
      "\n",
      "val auroc 69.73985764085006 \n",
      "\n",
      "\n",
      "===== EPOCH 14/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 3.7092719013040716 \n",
      "\n",
      "average train accuracy 95.19649621212122 \n",
      "\n",
      "train auroc 99.08229883306224 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.897553366999472 \n",
      "\n",
      "val accuracy 76.33487625776857 \n",
      "\n",
      "val auroc 69.41129665451987 \n",
      "\n",
      "\n",
      "===== EPOCH 15/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 3.445741040056402 \n",
      "\n",
      "average train accuracy 95.82149621212122 \n",
      "\n",
      "train auroc 99.31029919211608 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.448239664877614 \n",
      "\n",
      "val accuracy 81.80258582420835 \n",
      "\n",
      "val auroc 68.26264488098462 \n",
      "\n",
      "\n",
      "===== EPOCH 16/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 3.156993557106365 \n",
      "\n",
      "average train accuracy 96.2393465909091 \n",
      "\n",
      "train auroc 99.43549212902208 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.0469751819487545 \n",
      "\n",
      "val accuracy 80.157313554306 \n",
      "\n",
      "val auroc 66.93225731054254 \n",
      "\n",
      "\n",
      "===== EPOCH 17/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 2.8742811051282016 \n",
      "\n",
      "average train accuracy 96.46306818181819 \n",
      "\n",
      "train auroc 99.48748388080327 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.767889407373244 \n",
      "\n",
      "val accuracy 78.6414018570583 \n",
      "\n",
      "val auroc 68.27977096117208 \n",
      "\n",
      "\n",
      "===== EPOCH 18/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 2.6481986154209483 \n",
      "\n",
      "average train accuracy 96.94602272727273 \n",
      "\n",
      "train auroc 99.52645478800144 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.011544673673568 \n",
      "\n",
      "val accuracy 81.1259571988754 \n",
      "\n",
      "val auroc 68.26417391960635 \n",
      "\n",
      "\n",
      "===== EPOCH 19/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 2.452150583267212 \n",
      "\n",
      "average train accuracy 96.6595643939394 \n",
      "\n",
      "train auroc 99.55872952705407 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.718014809393114 \n",
      "\n",
      "val accuracy 78.39539712192956 \n",
      "\n",
      "val auroc 68.12973294207532 \n",
      "\n",
      "\n",
      "===== EPOCH 20/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 2.2476579308509828 \n",
      "\n",
      "average train accuracy 97.2549715909091 \n",
      "\n",
      "train auroc 99.65639073786437 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.803299088631907 \n",
      "\n",
      "val accuracy 80.47730467593964 \n",
      "\n",
      "val auroc 67.79625386455791 \n",
      "\n",
      "\n",
      "===== EPOCH 21/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 2.0959334834055467 \n",
      "\n",
      "average train accuracy 97.59588068181819 \n",
      "\n",
      "train auroc 99.71224912055729 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.528972379622921 \n",
      "\n",
      "val accuracy 79.1202325022196 \n",
      "\n",
      "val auroc 68.1237918917379 \n",
      "\n",
      "\n",
      "===== EPOCH 22/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.9138450384140016 \n",
      "\n",
      "average train accuracy 97.63494318181819 \n",
      "\n",
      "train auroc 99.72268896448728 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.820585220090805 \n",
      "\n",
      "val accuracy 80.59071193400415 \n",
      "\n",
      "val auroc 67.85235847375641 \n",
      "\n",
      "\n",
      "===== EPOCH 23/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.7844077809290453 \n",
      "\n",
      "average train accuracy 97.73910984848486 \n",
      "\n",
      "train auroc 99.72878867407081 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.721820577498405 \n",
      "\n",
      "val accuracy 80.33869580497189 \n",
      "\n",
      "val auroc 67.19384049385161 \n",
      "\n",
      "\n",
      "===== EPOCH 24/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.663872124932029 \n",
      "\n",
      "average train accuracy 97.97111742424242 \n",
      "\n",
      "train auroc 99.74637702378084 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.939079100085843 \n",
      "\n",
      "val accuracy 80.40828924977805 \n",
      "\n",
      "val auroc 67.85709532114457 \n",
      "\n",
      "\n",
      "===== EPOCH 25/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.5490865441885862 \n",
      "\n",
      "average train accuracy 97.77580492424242 \n",
      "\n",
      "train auroc 99.77525952242145 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.489722459546981 \n",
      "\n",
      "val accuracy 79.26323431488606 \n",
      "\n",
      "val auroc 67.50745579556522 \n",
      "\n",
      "\n",
      "===== EPOCH 26/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.4392742742191662 \n",
      "\n",
      "average train accuracy 98.00307765151514 \n",
      "\n",
      "train auroc 99.82339733184841 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.128398218462544 \n",
      "\n",
      "val accuracy 81.5297610239716 \n",
      "\n",
      "val auroc 67.56079023594444 \n",
      "\n",
      "\n",
      "===== EPOCH 27/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.3671390853144907 \n",
      "\n",
      "average train accuracy 98.0965909090909 \n",
      "\n",
      "train auroc 99.78477177274489 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 7.043808583290346 \n",
      "\n",
      "val accuracy 81.88038713376739 \n",
      "\n",
      "val auroc 66.79483562370623 \n",
      "\n",
      "\n",
      "===== EPOCH 28/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.2525862807577306 \n",
      "\n",
      "average train accuracy 98.14157196969697 \n",
      "\n",
      "train auroc 99.83276258384659 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.861209854002921 \n",
      "\n",
      "val accuracy 78.66001405741343 \n",
      "\n",
      "val auroc 67.05628884090402 \n",
      "\n",
      "\n",
      "===== EPOCH 29/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.19251450733705 \n",
      "\n",
      "average train accuracy 98.11316287878788 \n",
      "\n",
      "train auroc 99.82165965894211 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.288440781254923 \n",
      "\n",
      "val accuracy 80.05488865048831 \n",
      "\n",
      "val auroc 67.84520872447574 \n",
      "\n",
      "\n",
      "===== EPOCH 30/30 =====\n",
      "\n",
      "TRAINING...\n",
      "average train loss 1.1438785487955268 \n",
      "\n",
      "average train accuracy 98.34990530303031 \n",
      "\n",
      "train auroc 99.83010805882311 \n",
      "\n",
      "VALIDATION... \n",
      "\n",
      "val loss 6.953040938223562 \n",
      "\n",
      "val accuracy 80.9643431858538 \n",
      "\n",
      "val auroc 67.0222814056602 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('\\n===== EPOCH {}/{} ====='.format(epoch + 1, NUM_EPOCHS))    \n",
    "    print('\\nTRAINING...')\n",
    "\n",
    "    train_accuracy = [ ]\n",
    "    train_auroc = []\n",
    "    \n",
    "    train_loss = []\n",
    "    word2vec.train()\n",
    "    \n",
    "    for item1, item2 in zip(train_loader_sg, train_dataloader): \n",
    "        x_batch = item1[:,0]\n",
    "        y_batch = item1[:,1]\n",
    "        \n",
    "        # X is the input ids and X1 is other features\n",
    "        X, X1, y = item2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        loss_word2vec, logits = word2vec(x_batch, y_batch, X, X1)\n",
    "\n",
    "        \n",
    "        loss_cnn = criterion_cnn(logits,y)\n",
    "        loss =( 1-weight_cnn)*loss_word2vec + weight_cnn* loss_cnn  # * weight\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()    \n",
    "        \n",
    "                \n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        proba =logits[:,1].detach().numpy()\n",
    "\n",
    "\n",
    "        accuracy = (preds == y).cpu().numpy().mean() * 100\n",
    "        \n",
    "        \n",
    "        auroc =  roc_auc_score(y, proba)\n",
    "        \n",
    "        train_accuracy.append(accuracy)\n",
    "        train_auroc.append(auroc)\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"average train loss\", np.mean(train_loss), '\\n')\n",
    "    print(\"average train accuracy\", np.mean(train_accuracy), '\\n')\n",
    "    print(\"train auroc\", np.mean(train_auroc)*100, '\\n')\n",
    "    \n",
    "   \n",
    "    print(\"VALIDATION... \\n\")\n",
    "    val_loss, val_accuracy, val_auroc = evaluate(word2vec, val_dataloader, test_loader_sg, word2vec, criterion_cnn, weight_cnn)\n",
    "    print(\"val loss\", val_loss, '\\n')\n",
    "    print(\"val accuracy\", val_accuracy, '\\n')\n",
    "    print(\"val auroc\", np.mean(val_auroc)*100, '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
